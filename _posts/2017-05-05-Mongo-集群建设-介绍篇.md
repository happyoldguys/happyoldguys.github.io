---
layout: post
title: MongoDB 集群建设
date: 2017-05-05 18:20:00
tags: 技术
---
&emsp;&emsp;近日在实际项目环境中出现当游戏跨服查询所产生的连接数过多时会对后连接用户产生等待状态，但等待时间过长时会被自然抛弃；通过汇聚各方资料与研讨寻求出一套可以解决此项问题的方法，这就是MongoDB Sharding技术；因MongoDB Sharding的分片机制默认是自动、随机分片到各个shard上但为了满足现有环境的项目需求我们需要手动指定分片到哪个shard上，下面让我来介绍一下搭建方案与搭建方法。


##Sharding集群

####一、MongoDB集群角色

<img src="http://img.my.csdn.net/uploads/201303/27/1364349823_6398.png" style="display:block;margin:auto"/>

&emsp;&emsp;Mongod（实例）：多台实例会形成一组服务，而一组会形成集群；

&emsp;&emsp;ReplicaSet（副本集）：实现负载均衡与故障恢复，与主从复制不同的是，主从复制中只有一个Master，当Master节点挂掉之后，所有的Slave节点无法自动变为Master继续提供服务。这时所有的 Slave也就会因连不上Master而无法继续同步数据，直到它再次连到Master。

&emsp;&emsp;Shard（分片）：根据一组或多组（shards）区服的副本集通过Mongos在逻辑上创建出来的分片；数据分割按有序分割方式，每个分片上的数据为某一范围的数据块，故可支持指定分片的范围查询，数据块有指定的最大容量，一旦某个数据块的容量增长到最大容量时，这个数据块会切分成为两块；当分片的数据过多时，数据块将被迁移到系统的其他分片中。

&emsp;&emsp;Configsvr（配置服务器）：存储集群的metadata数据，数据中包含shards节点列表、chunk与数据集的映射关系信息等等；它为集群的枢纽部分，Mongos使用这些信息将请求路由到特定的shards上，对于production（生产）环境，一个集群必须有3个Configsvr。

&emsp;&emsp;Mongos（路由服务器）：查询路由节点，即Mongos节点，Mongos接收客户端请求，并负责将operations（操作）根据路由规则转发给合适的Shard或者Shards，然后再将result（结果）返回给客户端，它起到了一个路由、转发的作用，类似于proxy层。Sharded集群可以有多个Mongos节点，可以均衡客户端请求。

####二、办公环境集群建设

<img src="https://happyoldguys.github.io/assets/img/Mongo Sharding - 办公环境.jpeg" style="display:block;margin:auto"/>

######"搭建思路"：
><font face="楷体">&emsp;搭建前先确认项目需求与所投入主机数量从而选择横向与纵向模式，横向模式用于Master/Slave模式，shards相互备份起到异地备份作用，占用主机数量较少；纵向模式用于Master/Slave/Arbiter模式，每个角色占用一台主机起到一机多备并提供仲裁服务避免选举失败的问题，占用主机数量较多</font>

```
1. 建设Master/Slave模式，启动后检查主备状态，并在Master创建相应数据确认Slave能够正常同步
```
```
2.建设Configsvr副本集，启动后通过PRIMARY查看副本集状态，确认主机数量是否正常；设置各主机priority（优先）等级避免'0'的存在
```
```
3.建设Mongos在启动时需传递Configsvr副本集名与PRIMARY IP，成功启动后先添加shard主机、停止balancer（防止数据转移）、指定Databases分布，最后手动关闭分片功能并执行刷新操作
```

&emsp;&emsp;以上是横向模式的搭建过程，纵向模式与横向模式很相似在这里就不做过多赘述，下一篇文章会描述具体的搭建过程与相关命令。